---
title: "On AI and its Dissociation Effect"
date: 2025-07-09
author: "Ayanami Kaine"
tags: ["Programming", "AI"]
layout: ../../layouts/PostLayout.astro
summary: "When using AI tools to create something, like some boilerplate code I was to lazy to write or some win32 bindings I simply didnt know existed, I feel a form of dissociation. Like this part of my code base I simply dont own. Even when I rewrote broken parts that didnt work, added comments for things I didnt understand at first. It aways Remains"
---

*When using AI tools to create something, like some boilerplate code I was to lazy to write or some win32 bindings I simply didn't know existed, I feel a form of dissociation. Like this part of my code base I simply don't own. Even when I rewrote broken parts that didn't work, added comments for things I didn't understand at first. It's always remaining.*

## On AI

While this mono repository was initially created before the AI craze it was heavily pushed in VS-Code and VS in general. It was much easier to use it than to disable it by default. At first, it was really a nuisance gray auto-suggestion text popping up left and right. But after improvements in LLM models their coding abilities were at a stage where I felt that not using it results in a sever disadvantage. I saw so many people turning their ideas into apps that actually worked.

While I was fixing bugs left and right. My programming abilities and experience was limited to 2â€“3 years while LLMs already absorbed the knowledge of the past 50 years. While I needed to learn a new language it already understood it well enough to write moderately complex apps. Where I was struggling with the build system.

So I started using it more and more. Did it speed up my iteration speed? YES it did. My idea was being so fast implemented I could tweak problems with the idea much faster. The true value of an idea lies in its implementation not in the idea itself. But overtime i felt as if LLMs took over too much. It felt like I would lose the grasp of the idea itself.

While its true that the idea of something is decoupled from its implementation in the sense that an idea does not need an implementation, the implementation needs an idea to exist in the first place. Even if it's only a vague idea.

## The transformation of the idea

An implementation has the ability to transform an idea, it could expand or contract it. Imagine an idea of a cross-platform app for various platforms. The implementation could add a feature that won't work on phones. What does that have to do with LLMs? It does not matter of sophisticated a LLMs it could even be much more "smarter" than now it could never have the same idea in his "thoughts"/context window than you have in yours. This has something to do with words, meaning and context. Saying: "I want an app that works on Windows and Linux" and thinking and yea have the ability to easily add new platforms if I want to.

## Lost in translation

An LLM will never know your thoughts. And you might think: "duuuh, of course, it can't read your mind! You have to be more specific!". The thing is humans have much, much implicit behavior and meaning in speech and behavior in general, that you might think. Having an idea in your head and not the correct ability to translate it to speech does not make the idea less valuable. BUT! Translating something in speech/words/language is always a lossy operation, even if you don't believe that. We will lose context/meaning and even ideas itself. Saying: "I want that the app works". Can mean so many things in so many different contexts.

You can see the loose directly using code refactoring with an LLM it always adds concepts and ideas to your code you didn't think of and didn't use initially in the code base. Using 5 different ideas and ways to solve a problem it much worse than using 1 way to solve similar problems. In C# I personally prefer to use LINQ where I can that does not mean the LLM will do it. The number of thinks in my head that I don't think consciously about that influence the way I code. Would be too much to tell the LLM. And also I would miss important aspects.

When "talking" to an LLM it always means losing something. And I think I lost already too much.

## Losing too much

Using LLMs I lost understanding of my own code. Using LLMs I lost meaning in my ideas I wanted to implement because I thought a working idea is more important. Slowly but surely losing the idea itself in the process.
I also think I lost valuable learning experiences.

This is one of the reasons I won't let an LLM touch my blog's content, even to correct spelling or grammar mistakes. Because doing so would remove me from my own blog. I have make mistakes, but making mistakes is human. And my mistakes are something that makes me, me. Removing all possible mistakes and errors feels like making my brain smooth.

![Smooth brain meme](/smoth_brain.webp)


## So never use an LLM?

NO, this is not the conclusion to take. The main conclusion is that it should not generate code whatsoever. It pollutes the implementation way too much. I found that LLMs are sometimes much better than an online search. Or for translating code to another language.

## Iteration Speed

AI really increases the iteration speed. Building many small programs and cutting the cruft of many languages. But using it for more complex system where one part after another is build using AI, that does not have a clear concept/overview of the system itself. 

The other problem is that the first output of an LLM is seldom right, it needs personal fixes and changes. 

## The conclusion

In the end a LLM is nothing more than a tool, some compare it to a junior dev. I strongly disagree, a LLM is inhumane. It does not have unique ideas in his model is all averaged out. Often providing just one perspective. I wonder what the consequences are for large scale projects where one model is extensively used.
